{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparikfy churn\") \\\n",
    "    .config(\"spark.default.parallelism\", 1000) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.default.parallelism', '1000'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.shuffle.partitions', '8'),\n",
       " ('spark.app.name', 'Sparikfy churn'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"mini_sparkify_event_data.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.createOrReplaceTempView(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "\n",
      "+--------+\n",
      "|    page|\n",
      "+--------+\n",
      "|NextSong|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for any missing userId's or sessionId's, none\n",
    "spark.sql('''\n",
    "        select *\n",
    "        from data\n",
    "\n",
    "\n",
    "''').printSchema()\n",
    "\n",
    "spark.sql('''\n",
    "        select *\n",
    "        from data\n",
    "        where userId is null or sessionId is null\n",
    "''').show()\n",
    "\n",
    "#noticed that there were null songs, that is when page's value is not'next song'\n",
    "spark.sql('''\n",
    "        select distinct page\n",
    "        from data\n",
    "        where song is not null\n",
    "''').show(40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>277.89016</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Iron Frenzy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>Long</td>\n",
       "      <td>236.09424</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352180000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adam Lambert</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>282.82730</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Time For Miracles</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352394000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enigma</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>80</td>\n",
       "      <td>Long</td>\n",
       "      <td>262.71302</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Knocking On Forbidden Doors</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352416000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daft Punk</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>52</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>223.60771</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Harder Better Faster Stronger</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352676000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist       auth firstName gender  itemInSession lastName  \\\n",
       "0    Martha Tilston  Logged In     Colin      M             50  Freeman   \n",
       "1  Five Iron Frenzy  Logged In     Micah      M             79     Long   \n",
       "2      Adam Lambert  Logged In     Colin      M             51  Freeman   \n",
       "3            Enigma  Logged In     Micah      M             80     Long   \n",
       "4         Daft Punk  Logged In     Colin      M             52  Freeman   \n",
       "\n",
       "      length level                        location method      page  \\\n",
       "0  277.89016  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "1  236.09424  free  Boston-Cambridge-Newton, MA-NH    PUT  NextSong   \n",
       "2  282.82730  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "3  262.71302  free  Boston-Cambridge-Newton, MA-NH    PUT  NextSong   \n",
       "4  223.60771  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "\n",
       "   registration  sessionId                           song  status  \\\n",
       "0  1.538173e+12         29                      Rockpools     200   \n",
       "1  1.538332e+12          8                         Canada     200   \n",
       "2  1.538173e+12         29              Time For Miracles     200   \n",
       "3  1.538332e+12          8    Knocking On Forbidden Doors     200   \n",
       "4  1.538173e+12         29  Harder Better Faster Stronger     200   \n",
       "\n",
       "              ts                                          userAgent userId  \n",
       "0  1538352117000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "1  1538352180000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  \n",
       "2  1538352394000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "3  1538352416000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  \n",
       "4  1538352676000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        select *\n",
    "        from data\n",
    "\n",
    "\n",
    "''').toPandas().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"minute\", lambda x: str(datetime.datetime.fromtimestamp(x / 1000.0).minute))\n",
    "spark.udf.register(\"seconds\", lambda x: str(datetime.datetime.fromtimestamp(x / 1000.0).second))\n",
    "spark.udf.register(\"date\", lambda x: str(datetime.datetime.fromtimestamp(x / 1000.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, minute: string, seconds: string, date: string, registrationdate: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        select *,minute(ts) as minute,seconds(ts) as seconds, date(ts) as date, date(registration) as registrationdate\n",
    "        from data\n",
    "        where userId != ''\n",
    "        order by userId, sessionId, ts asc\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the ratio of time spent listening to songs vs spent in the in between stages\n",
    "time_data = spark.sql('''\n",
    "          select *, unix_timestamp(datesntimes) - unix_timestamp(previous_datetime) as timediffinsecs\n",
    "              From (select *, LAG(datesntimes) OVER (\n",
    "                PARTITION BY userid, sessionid \n",
    "                ORDER BY datesntimes) previous_datetime\n",
    "                from (\n",
    "                    select userId, sessionId,page, CAST(date(ts) as timestamp) as datesntimes\n",
    "                    from data\n",
    "                    where userId != ''\n",
    "                    order by userId, sessionId, ts asc))\n",
    "    \n",
    "                order by datesntimes\n",
    "               \n",
    "''')\n",
    "\n",
    "time_data.createOrReplaceTempView(\"time_data\")\n",
    "\n",
    "\n",
    "listening_ratio = spark.sql(\"\"\"\n",
    "        \n",
    "        select userid, ifnull(sum(total_time_non_listening),0) as no_listen_time, ifnull(sum(total_time_per_session),0) as total_time, ifnull(sum(total_time_non_listening/total_time_per_session),0) as listening_ratio\n",
    "        from(\n",
    "            select abc.userid, abc.sessionid, abc.total_time_per_session, b.total_time_non_listening\n",
    "            from\n",
    "                (select userid, sessionid, sum(timediffinsecs) as total_time_per_session\n",
    "                from time_data\n",
    "                group by userid, sessionid\n",
    "                order by sessionid) abc\n",
    "\n",
    "                left join (select userid, sessionid, sum(timediffinsecs) as total_time_non_listening\n",
    "                    from time_data\n",
    "                    where page != 'NextSong'\n",
    "                    group by userid, sessionid\n",
    "                    order by sessionid) as b \n",
    "            on abc.userid = b.userid and abc.sessionid = b.sessionid\n",
    "            order by userid, sessionid asc)\n",
    "           group by userid\n",
    "           order by userid\n",
    "        \n",
    "\"\"\")\n",
    "listening_ratio.createOrReplaceTempView(\"listening_vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning the data into one user per row so that I can create a vector to use for the models\n",
    "\n",
    "pivot_data = spark.sql('''\n",
    "        \n",
    "        select userId, sessionId, page, count(page) as page_count\n",
    "        from data\n",
    "        where userId != \"\"\n",
    "        group by userId, sessionId, page\n",
    "        order by userId, sessionId, page_count desc\n",
    "        \n",
    "\n",
    "''')\n",
    "pivot_data.createOrReplaceTempView(\"pivot_data\")\n",
    "\n",
    "transformed_data = spark.sql('''\n",
    "            select userId, sessionId,\n",
    "                sum(CancellationConfirmation) as CancellationConfirmation,\n",
    "                sum(SubmitUpgrade) as SubmitUpgrade,\n",
    "                sum(Upgrade) as Upgrade,\n",
    "                sum(Help) as Help,\n",
    "                sum(ThumbsUp) as ThumbsUp,\n",
    "                sum(NextSong) as NextSong,\n",
    "                sum(AddFriend) as AddFriend,\n",
    "                sum(AddtoPlaylist) as AddtoPlaylist,\n",
    "                sum(Settings) as Settings,\n",
    "                sum(About) as About,\n",
    "                sum(SaveSettings) as SaveSettings,\n",
    "                sum(Logout) as Logout,\n",
    "                sum(RollAdvert) as RollAdvert,\n",
    "                sum(Downgrade) as Downgrade,\n",
    "                sum(Home) as Home,\n",
    "                sum(ThumbsDown) as ThumbsDown,\n",
    "                sum(SubmitDowngrade) as SubmitDowngrade,\n",
    "                sum(Cancel) as Cancel\n",
    "                \n",
    "                from (select pivot_data.*,\n",
    "                    case when page = \"Cancellation Confirmation\" then page_count else 0 end as CancellationConfirmation,\n",
    "                    case when page = \"Submit Upgrade\" then page_count  else 0 end as SubmitUpgrade,\n",
    "                    case when page = \"Upgrade\" then page_count  else 0 end as Upgrade,\n",
    "                    case when page = \"Help\" then page_count  else 0 end as Help,\n",
    "                    case when page = \"Thumbs Up\" then page_count  else 0 end as ThumbsUp,\n",
    "                    case when page = \"NextSong\" then page_count  else 0 end as NextSong,\n",
    "                    case when page = \"Add Friend\" then page_count  else 0 end as AddFriend,\n",
    "                    case when page = \"Add to Playlist\" then page_count  else 0 end as AddtoPlaylist,\n",
    "                    case when page = \"Settings\" then page_count  else 0 end as Settings,\n",
    "                    case when page = \"About\" then page_count  else 0 end as About,\n",
    "                    case when page = \"Save Settings\" then page_count  else 0 end as SaveSettings,\n",
    "                    case when page = \"Logout\" then page_count  else 0 end as Logout,\n",
    "                    case when page = \"Roll Advert\" then page_count  else 0 end as RollAdvert,\n",
    "                    case when page = \"Downgrade\" then page_count  else 0 end as Downgrade,\n",
    "                    case when page = \"Home\" then page_count  else 0 end as Home,\n",
    "                    case when page = \"Thumbs Down\" then page_count  else 0 end as ThumbsDown,\n",
    "                    case when page = \"Submit Downgrade\" then page_count  else 0 end as SubmitDowngrade,\n",
    "                    case when page = \"Cancel\" then page_count  else 0 end as Cancel\n",
    "                from pivot_data) pivot_ext\n",
    "                group by userId, sessionId\n",
    "                order by userId, sessionId asc\n",
    "''')\n",
    "transformed_data.createOrReplaceTempView(\"transformed_vectors\")\n",
    "\n",
    "\n",
    "\n",
    "transform_vector = spark.sql('''\n",
    "        select userId, sum(CancellationConfirmation) as CancellationConfirmation,\n",
    "                sum(SubmitUpgrade) as SubmitUpgrade,\n",
    "                sum(Upgrade) as Upgrade,\n",
    "                sum(Help) as Help,\n",
    "                sum(ThumbsUp) as ThumbsUp,\n",
    "                sum(NextSong) as NextSong,\n",
    "                sum(AddFriend) as AddFriend,\n",
    "                sum(AddtoPlaylist) as AddtoPlaylist,\n",
    "                sum(Settings) as Settings,\n",
    "                sum(About) as About,\n",
    "                sum(SaveSettings) as SaveSettings,\n",
    "                sum(Logout) as Logout,\n",
    "                sum(RollAdvert) as RollAdvert,\n",
    "                sum(Downgrade) as Downgrade,\n",
    "                sum(Home) as Home,\n",
    "                sum(ThumbsDown) as ThumbsDown,\n",
    "                sum(SubmitDowngrade) as SubmitDowngrade,\n",
    "                sum(Cancel) as Cancel\n",
    "        from transformed_vectors\n",
    "        group by userId\n",
    "''')\n",
    "transform_vector.createOrReplaceTempView(\"transform_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#number of sessions each user has completed\n",
    "num_sessions_vector = spark.sql('''\n",
    "        \n",
    "        select userId, count(*) as sessions\n",
    "        FROM (select distinct userId, sessionId\n",
    "                from data\n",
    "                where userId != \"\")\n",
    "        group by userId\n",
    "        order by sessions Desc\n",
    "        \n",
    "\n",
    "''')\n",
    "\n",
    "num_sessions_vector.createOrReplaceTempView('num_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring all the different engineered features into one table\n",
    "\n",
    "feature_engineered = spark.sql('''\n",
    "                select trans.*, listen.no_listen_time, listen.total_time, listen.listening_ratio, num_sess.sessions \n",
    "                    from listening_vectors listen\n",
    "                left join num_vectors num_sess\n",
    "                on listen.userid = num_sess.userid\n",
    "                left join transform_vector trans\n",
    "                on trans.userid = listen.userid\n",
    "                where listen.no_listen_time is not null or listen.total_time is not null                \n",
    "''')\n",
    "\n",
    "#user 135 only has 1 session and they listen to 6 songs but engage in no other activities, so I removed the id\n",
    "# with the WHERE statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "ignore = ['userId',\"CancellationConfirmation\", 'Cancel' ]\n",
    "\n",
    "#removing Cancel column, as they have to click cancel to get to cancellation, an unavoidable step in cancelling\n",
    "#if they are are hitting the cancel page, it is nearly 100% that they will follow through with cancellation.\n",
    "\n",
    "\n",
    "assembler_page = VectorAssembler(\n",
    "  inputCols=[x for x in feature_engineered.columns if x not in ignore], outputCol=\"features\"\n",
    ")\n",
    "\n",
    "assembled = assembler_page.transform(feature_engineered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final = assembled.select(['userId','features',\"CancellationConfirmation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assembled.toPandas().describe()\n",
    "#the ratio of cancellations to non cancellations is 0.23111 which will be the initial weights for the churn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = final.randomSplit([0.7, 0.3], seed=42) #42 seed for test\n",
    "train_df = splits[0]\n",
    "test_val = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    118\n",
      "1     34\n",
      "Name: CancellationConfirmation, dtype: int64\n",
      "0    55\n",
      "1    18\n",
      "Name: CancellationConfirmation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking the ratio of the different classes to make sure they are the same\n",
    "\n",
    "print (train_df.toPandas()['CancellationConfirmation'].value_counts())\n",
    "\n",
    "print (test_val.toPandas()['CancellationConfirmation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scaling the vectors so that they can be used with the logisitic regression classifier\n",
    "#seperating the train and test so there are no data leak after its scaled\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "scalerModel = scaler.fit(train_df)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsampling, as an option for the larger dataset\n",
    "\n",
    "train_df_sampled = scaledData.sampleBy(\"CancellationConfirmation\", fractions= {0: 1.0, 1: 1.0 })\n",
    "train_df_sampled.createOrReplaceTempView(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#adding weights\n",
    "spark.udf.register(\"weight\", lambda x: (x+1)*0.23 if x == 0 else 1)\n",
    "\n",
    "train_df_model = spark.sql('''\n",
    "            select *, cast(weight(CancellationConfirmation) as float) as weight\n",
    "            from train\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression( featuresCol=\"scaledFeatures\", labelCol=\"CancellationConfirmation\", weightCol=\"weight\")\n",
    "dt = DecisionTreeClassifier(labelCol=\"CancellationConfirmation\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"CancellationConfirmation\", metricName= \"f1\")\n",
    "\n",
    "pipeline = Pipeline(stages=[]) \n",
    "\n",
    "lr_stages = [lr]\n",
    "dt_stages = [dt]\n",
    "\n",
    "lr_pipeline = Pipeline(stages=lr_stages)\n",
    "dt_pipeline = Pipeline(stages=dt_stages)\n",
    "\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .baseOn({pipeline.stages:lr_stages})  \\\n",
    "    .addGrid(lr.maxIter, [200, ]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0]) \\\n",
    "    .addGrid(lr.threshold, [0.6]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "paramGrid_dt = ParamGridBuilder() \\\n",
    "    .baseOn({pipeline.stages:dt_stages})\\\n",
    "    .addGrid(dt.maxDepth, [ 10, 15]) \\\n",
    "    .addGrid(dt.maxBins, [4,5,6]) \\\n",
    "    .build()\n",
    "\n",
    "paramGrid = paramGrid_lr #+ paramGrid_dt\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) \n",
    "\n",
    "cvModel = crossval.fit(train_df_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledtest = scalerModel.transform(test_val)\n",
    "\n",
    "prediction = cvModel.transform(scaledtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_1a7e4a0f4c38"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"trainingsummary = cvModel.bestModel.summary\\nimport matplotlib.pyplot as plt\\nplt.figure(figsize=(5,5))\\nplt.plot([0, 1], [0, 1], 'r--')\\nplt.plot(trainingsummary.pr.select('recall').collect(),\\n         trainingsummary.pr.select('precision').collect())\\nplt.xlabel('recall')\\nplt.ylabel('precision')\\nplt.show()\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''trainingsummary = cvModel.bestModel.summary\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(trainingsummary.pr.select('recall').collect(),\n",
    "         trainingsummary.pr.select('precision').collect())\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_df009274b5b6', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='featuresCol', doc='features column name'): 'scaledFeatures',\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='labelCol', doc='label column name'): 'CancellationConfirmation',\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='maxIter', doc='maximum number of iterations (>= 0)'): 200,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='regParam', doc='regularization parameter (>= 0)'): 0.0,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.6,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06,\n",
       " Param(parent='LogisticRegression_df009274b5b6', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0'): 'weight'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPipeline = cvModel.bestModel.stages[-1]\n",
    "\n",
    "bestParams = bestPipeline.extractParamMap()\n",
    "bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for validation set is 0.7014643363249882\n",
      "The recall for validation set is 0.6301369863013698\n",
      "The f1 for validation set is 0.6528708831244536\n"
     ]
    }
   ],
   "source": [
    "print (\"The precision for validation set is {}\".format(evaluator.evaluate(prediction, {evaluator.metricName: \"weightedPrecision\"})))\n",
    "print (\"The recall for validation set is {}\".format(evaluator.evaluate(prediction, {evaluator.metricName: \"weightedRecall\"})))\n",
    "print (\"The f1 for validation set is {}\".format(evaluator.evaluate(prediction, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Pipeline: With the Decision Tree:\n",
    "\n",
    "The precision for validation set is 0.6536680911680912\n",
    "\n",
    "The recall for validation set is 0.6025641025641025\n",
    "\n",
    "The f1 for validation set is 0.624395682083911\n",
    "\n",
    "- With the Logistic Regression\n",
    "\n",
    "The precision for validation set is 0.743443033745406\n",
    "\n",
    "The recall for validation set is 0.5753424657534246\n",
    "\n",
    "The f1 for validation set is 0.6019818045469951\n",
    "\n",
    "False positives: 27 \n",
    "False negatives: 4 \n",
    "True Negatives: 28 \n",
    "True positives: 14 \n",
    "\n",
    "\n",
    "- Tested Logistic Regression threshold change to 0.7\n",
    "\n",
    "The precision for validation set is 0.7069615517439853\n",
    "The recall for validation set is 0.7123287671232876\n",
    "The f1 for validation set is 0.7095149944465013\n",
    "False positives: 10 \n",
    "False negatives: 11 \n",
    "True Negatives: 45 \n",
    "True positives: 7 \n",
    "\n",
    "- Threshold to 0.8\n",
    "The precision for validation set is 0.7212437486410088\n",
    "The recall for validation set is 0.7534246575342465\n",
    "The f1 for validation set is 0.7265580947958473\n",
    "\n",
    "The F1 score increased but it focusing to much on the recall at 0.8 threshold, the userids which are showing up as possible churners have much newer ids. I dont particularly want to make newer users, and the super high probability users the only ones who are at risk. My concern is that low usage stats are being identified as high churn, which doesn't really discern between new and old users. I think adding in the registration date as a feature would help correct some of this. \n",
    "False positives: 5 \n",
    "False negatives: 13 \n",
    "True Negatives: 50 \n",
    "True positives: 5 \n",
    "\n",
    "- Threshold to 0.6\n",
    "\n",
    "The precision for validation set is 0.7014643363249882\n",
    "The recall for validation set is 0.6301369863013698\n",
    "The f1 for validation set is 0.6528708831244536\n",
    "\n",
    "Testing a lower threshold. I think this threshold may be the best, when the full dataset is looked at, it maybe easier to see where to put the threshold since we will have more examples of churn. Further more, we can examine them more by probability further. I believe that this will give a small enough group to work with. \n",
    "False positives: 19 \n",
    "False negatives: 8 \n",
    "True Negatives: 36 \n",
    "True positives: 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "False positives: 19 \n",
      "False negatives: 8 \n",
      "True Negatives: 36 \n",
      "True positives: 10 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>features</th>\n",
       "      <th>CancellationConfirmation</th>\n",
       "      <th>scaledFeatures</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100010</td>\n",
       "      <td>[0.0, 2.0, 2.0, 17.0, 275.0, 4.0, 7.0, 0.0, 1....</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.812821832988, 0.267284637662, 0.247282...</td>\n",
       "      <td>[-1.21331093848, 1.21331093848]</td>\n",
       "      <td>[0.229115742889, 0.770884257111]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>(0.0, 0.0, 0.0, 6.0, 158.0, 0.0, 1.0, 1.0, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0872763454234, 0.13699157192...</td>\n",
       "      <td>[-1.47304005215, 1.47304005215]</td>\n",
       "      <td>[0.186480980716, 0.813519019284]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300014</td>\n",
       "      <td>[1.0, 1.0, 4.0, 27.0, 280.0, 3.0, 10.0, 2.0, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.398520126, 0.406410916494, 0.534569275324, ...</td>\n",
       "      <td>[-1.10764255108, 1.10764255108]</td>\n",
       "      <td>[0.248310651094, 0.751689348906]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>[0.0, 0.0, 10.0, 54.0, 1230.0, 19.0, 34.0, 6.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 1.33642318831, 0.78548710881, 1.066...</td>\n",
       "      <td>[-0.445871248472, 0.445871248472]</td>\n",
       "      <td>[0.390342860948, 0.609657139052]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200004</td>\n",
       "      <td>[0.0, 0.0, 9.0, 85.0, 1742.0, 21.0, 67.0, 18.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 1.20278086948, 1.2364148935, 1.5103...</td>\n",
       "      <td>[-6.96441187177, 6.96441187177]</td>\n",
       "      <td>[0.000944026477394, 0.999055973523]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>89</td>\n",
       "      <td>[1.0, 2.0, 5.0, 30.0, 667.0, 14.0, 12.0, 10.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.398520126, 0.812821832988, 0.668211594155, ...</td>\n",
       "      <td>[-0.469074766393, 0.469074766393]</td>\n",
       "      <td>[0.384835257361, 0.615164742639]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>66</td>\n",
       "      <td>[1.0, 3.0, 3.0, 52.0, 1045.0, 24.0, 38.0, 5.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.398520126, 1.21923274948, 0.400926956493, 0...</td>\n",
       "      <td>[-0.683947926694, 0.683947926694]</td>\n",
       "      <td>[0.335380736629, 0.664619263371]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>200006</td>\n",
       "      <td>[0.0, 2.0, 1.0, 21.0, 542.0, 8.0, 17.0, 2.0, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.812821832988, 0.133642318831, 0.305467...</td>\n",
       "      <td>[-0.632388879756, 0.632388879756]</td>\n",
       "      <td>[0.346969064081, 0.653030935919]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>63</td>\n",
       "      <td>[0.0, 1.0, 0.0, 6.0, 87.0, 0.0, 1.0, 1.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.406410916494, 0.0, 0.0872763454234, 0....</td>\n",
       "      <td>[-0.63486352761, 0.63486352761]</td>\n",
       "      <td>[0.346408567122, 0.653591432878]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>83</td>\n",
       "      <td>[1.0, 4.0, 10.0, 69.0, 1235.0, 18.0, 28.0, 11....</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.398520126, 1.62564366598, 1.33642318831, 1....</td>\n",
       "      <td>[-2.69648127975, 2.69648127975]</td>\n",
       "      <td>[0.0631813072572, 0.936818692743]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>94</td>\n",
       "      <td>[0.0, 1.0, 0.0, 4.0, 146.0, 1.0, 3.0, 2.0, 1.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.406410916494, 0.0, 0.0581842302822, 0....</td>\n",
       "      <td>[-0.460422407613, 0.460422407613]</td>\n",
       "      <td>[0.386885621819, 0.613114378181]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>123</td>\n",
       "      <td>[0.0, 1.0, 1.0, 5.0, 150.0, 5.0, 8.0, 1.0, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.406410916494, 0.133642318831, 0.072730...</td>\n",
       "      <td>[-1.37540494245, 1.37540494245]</td>\n",
       "      <td>[0.201748000126, 0.798251999874]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>142</td>\n",
       "      <td>[1.0, 1.0, 15.0, 111.0, 1875.0, 28.0, 50.0, 14...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.398520126, 0.406410916494, 2.00463478247, 1...</td>\n",
       "      <td>[-1.18093312915, 1.18093312915]</td>\n",
       "      <td>[0.234884458613, 0.765115541387]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>200014</td>\n",
       "      <td>[1.0, 1.0, 5.0, 43.0, 747.0, 20.0, 25.0, 5.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.398520126, 0.406410916494, 0.668211594155, ...</td>\n",
       "      <td>[-3.20568883905, 3.20568883905]</td>\n",
       "      <td>[0.0389522020643, 0.961047797936]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>200023</td>\n",
       "      <td>[3.0, 15.0, 23.0, 163.0, 2955.0, 66.0, 73.0, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.19556037799, 6.09616374741, 3.07377333311, ...</td>\n",
       "      <td>[-0.695772248917, 0.695772248917]</td>\n",
       "      <td>[0.332750240254, 0.667249759746]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>119</td>\n",
       "      <td>[0.0, 2.0, 3.0, 7.0, 173.0, 4.0, 5.0, 3.0, 1.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.812821832988, 0.400926956493, 0.101822...</td>\n",
       "      <td>[-0.446708633292, 0.446708633292]</td>\n",
       "      <td>[0.390143602342, 0.609856397658]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>200022</td>\n",
       "      <td>[0.0, 2.0, 2.0, 16.0, 348.0, 6.0, 5.0, 2.0, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.812821832988, 0.267284637662, 0.232736...</td>\n",
       "      <td>[-1.95577342724, 1.95577342724]</td>\n",
       "      <td>[0.1239251881, 0.8760748119]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>134</td>\n",
       "      <td>[0.0, 1.0, 0.0, 3.0, 37.0, 0.0, 2.0, 1.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.406410916494, 0.0, 0.0436381727117, 0....</td>\n",
       "      <td>[-0.92131717825, 0.92131717825]</td>\n",
       "      <td>[0.284689586188, 0.715310413812]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>15</td>\n",
       "      <td>[0.0, 0.0, 8.0, 81.0, 1914.0, 31.0, 59.0, 16.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 1.06913855065, 1.17823066322, 1.659...</td>\n",
       "      <td>[-0.653574437529, 0.653574437529]</td>\n",
       "      <td>[0.342184498662, 0.657815501338]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId                                           features  \\\n",
       "0   100010  [0.0, 2.0, 2.0, 17.0, 275.0, 4.0, 7.0, 0.0, 1....   \n",
       "2      102  (0.0, 0.0, 0.0, 6.0, 158.0, 0.0, 1.0, 1.0, 0.0...   \n",
       "4   300014  [1.0, 1.0, 4.0, 27.0, 280.0, 3.0, 10.0, 2.0, 1...   \n",
       "7       14  [0.0, 0.0, 10.0, 54.0, 1230.0, 19.0, 34.0, 6.0...   \n",
       "8   200004  [0.0, 0.0, 9.0, 85.0, 1742.0, 21.0, 67.0, 18.0...   \n",
       "12      89  [1.0, 2.0, 5.0, 30.0, 667.0, 14.0, 12.0, 10.0,...   \n",
       "17      66  [1.0, 3.0, 3.0, 52.0, 1045.0, 24.0, 38.0, 5.0,...   \n",
       "23  200006  [0.0, 2.0, 1.0, 21.0, 542.0, 8.0, 17.0, 2.0, 1...   \n",
       "28      63  [0.0, 1.0, 0.0, 6.0, 87.0, 0.0, 1.0, 1.0, 0.0,...   \n",
       "29      83  [1.0, 4.0, 10.0, 69.0, 1235.0, 18.0, 28.0, 11....   \n",
       "31      94  [0.0, 1.0, 0.0, 4.0, 146.0, 1.0, 3.0, 2.0, 1.0...   \n",
       "37     123  [0.0, 1.0, 1.0, 5.0, 150.0, 5.0, 8.0, 1.0, 0.0...   \n",
       "40     142  [1.0, 1.0, 15.0, 111.0, 1875.0, 28.0, 50.0, 14...   \n",
       "41  200014  [1.0, 1.0, 5.0, 43.0, 747.0, 20.0, 25.0, 5.0, ...   \n",
       "43  200023  [3.0, 15.0, 23.0, 163.0, 2955.0, 66.0, 73.0, 1...   \n",
       "55     119  [0.0, 2.0, 3.0, 7.0, 173.0, 4.0, 5.0, 3.0, 1.0...   \n",
       "59  200022  [0.0, 2.0, 2.0, 16.0, 348.0, 6.0, 5.0, 2.0, 0....   \n",
       "68     134  [0.0, 1.0, 0.0, 3.0, 37.0, 0.0, 2.0, 1.0, 0.0,...   \n",
       "69      15  [0.0, 0.0, 8.0, 81.0, 1914.0, 31.0, 59.0, 16.0...   \n",
       "\n",
       "    CancellationConfirmation  \\\n",
       "0                          0   \n",
       "2                          0   \n",
       "4                          0   \n",
       "7                          0   \n",
       "8                          0   \n",
       "12                         0   \n",
       "17                         0   \n",
       "23                         0   \n",
       "28                         0   \n",
       "29                         0   \n",
       "31                         0   \n",
       "37                         0   \n",
       "40                         0   \n",
       "41                         0   \n",
       "43                         0   \n",
       "55                         0   \n",
       "59                         0   \n",
       "68                         0   \n",
       "69                         0   \n",
       "\n",
       "                                       scaledFeatures  \\\n",
       "0   [0.0, 0.812821832988, 0.267284637662, 0.247282...   \n",
       "2   (0.0, 0.0, 0.0, 0.0872763454234, 0.13699157192...   \n",
       "4   [1.398520126, 0.406410916494, 0.534569275324, ...   \n",
       "7   [0.0, 0.0, 1.33642318831, 0.78548710881, 1.066...   \n",
       "8   [0.0, 0.0, 1.20278086948, 1.2364148935, 1.5103...   \n",
       "12  [1.398520126, 0.812821832988, 0.668211594155, ...   \n",
       "17  [1.398520126, 1.21923274948, 0.400926956493, 0...   \n",
       "23  [0.0, 0.812821832988, 0.133642318831, 0.305467...   \n",
       "28  [0.0, 0.406410916494, 0.0, 0.0872763454234, 0....   \n",
       "29  [1.398520126, 1.62564366598, 1.33642318831, 1....   \n",
       "31  [0.0, 0.406410916494, 0.0, 0.0581842302822, 0....   \n",
       "37  [0.0, 0.406410916494, 0.133642318831, 0.072730...   \n",
       "40  [1.398520126, 0.406410916494, 2.00463478247, 1...   \n",
       "41  [1.398520126, 0.406410916494, 0.668211594155, ...   \n",
       "43  [4.19556037799, 6.09616374741, 3.07377333311, ...   \n",
       "55  [0.0, 0.812821832988, 0.400926956493, 0.101822...   \n",
       "59  [0.0, 0.812821832988, 0.267284637662, 0.232736...   \n",
       "68  [0.0, 0.406410916494, 0.0, 0.0436381727117, 0....   \n",
       "69  [0.0, 0.0, 1.06913855065, 1.17823066322, 1.659...   \n",
       "\n",
       "                        rawPrediction                          probability  \\\n",
       "0     [-1.21331093848, 1.21331093848]     [0.229115742889, 0.770884257111]   \n",
       "2     [-1.47304005215, 1.47304005215]     [0.186480980716, 0.813519019284]   \n",
       "4     [-1.10764255108, 1.10764255108]     [0.248310651094, 0.751689348906]   \n",
       "7   [-0.445871248472, 0.445871248472]     [0.390342860948, 0.609657139052]   \n",
       "8     [-6.96441187177, 6.96441187177]  [0.000944026477394, 0.999055973523]   \n",
       "12  [-0.469074766393, 0.469074766393]     [0.384835257361, 0.615164742639]   \n",
       "17  [-0.683947926694, 0.683947926694]     [0.335380736629, 0.664619263371]   \n",
       "23  [-0.632388879756, 0.632388879756]     [0.346969064081, 0.653030935919]   \n",
       "28    [-0.63486352761, 0.63486352761]     [0.346408567122, 0.653591432878]   \n",
       "29    [-2.69648127975, 2.69648127975]    [0.0631813072572, 0.936818692743]   \n",
       "31  [-0.460422407613, 0.460422407613]     [0.386885621819, 0.613114378181]   \n",
       "37    [-1.37540494245, 1.37540494245]     [0.201748000126, 0.798251999874]   \n",
       "40    [-1.18093312915, 1.18093312915]     [0.234884458613, 0.765115541387]   \n",
       "41    [-3.20568883905, 3.20568883905]    [0.0389522020643, 0.961047797936]   \n",
       "43  [-0.695772248917, 0.695772248917]     [0.332750240254, 0.667249759746]   \n",
       "55  [-0.446708633292, 0.446708633292]     [0.390143602342, 0.609856397658]   \n",
       "59    [-1.95577342724, 1.95577342724]         [0.1239251881, 0.8760748119]   \n",
       "68    [-0.92131717825, 0.92131717825]     [0.284689586188, 0.715310413812]   \n",
       "69  [-0.653574437529, 0.653574437529]     [0.342184498662, 0.657815501338]   \n",
       "\n",
       "    prediction  \n",
       "0          1.0  \n",
       "2          1.0  \n",
       "4          1.0  \n",
       "7          1.0  \n",
       "8          1.0  \n",
       "12         1.0  \n",
       "17         1.0  \n",
       "23         1.0  \n",
       "28         1.0  \n",
       "29         1.0  \n",
       "31         1.0  \n",
       "37         1.0  \n",
       "40         1.0  \n",
       "41         1.0  \n",
       "43         1.0  \n",
       "55         1.0  \n",
       "59         1.0  \n",
       "68         1.0  \n",
       "69         1.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaya = prediction.toPandas()\n",
    "\n",
    "print (len(yaya))\n",
    "print (\"False positives: {} \".format(len(yaya[(yaya['prediction'] == 1) & (yaya['CancellationConfirmation'] == 0)])))\n",
    "print (\"False negatives: {} \".format(len(yaya[(yaya['prediction'] == 0) & (yaya['CancellationConfirmation'] == 1)])))\n",
    "print (\"True Negatives: {} \".format(len(yaya[(yaya['prediction'] == 0) & (yaya['CancellationConfirmation'] == 0)])))\n",
    "print (\"True positives: {} \".format(len(yaya[(yaya['prediction'] == 1) & (yaya['CancellationConfirmation'] == 1)])))\n",
    "\n",
    "yaya[(yaya['prediction'] == 1) & (yaya['CancellationConfirmation'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#to see the values for the decision tree\n",
    "#print (bestPipeline.toDebugString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
